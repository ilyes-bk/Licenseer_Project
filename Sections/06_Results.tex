\vspace{-.1cm}
\section{Experimental Results}
\label{Section:Result}

In this section, we outline our experimental setup, structured according to the framework presented in \cite{LwakatareRCBO20}, to rigorously evaluate our approach. We describe the evaluation metrics, baseline methods, and implementation procedures, and then present a detailed analysis of the experimental results, highlighting the effectiveness and practical implications of our methodology.

\subsection{Data}

Our dataset collection employed a multi-stage sampling strategy to ensure representative coverage of the open-source software landscape. Python projects on GitHub were selected using stratified sampling across several dimensions, including project size (small: $<$100 dependencies, 25\%; medium: 100–500 dependencies, 45\%; large: $>$500 dependencies, 30\%), domain categories (Web Development 32\%, Machine Learning 18\%, System Utilities 15\%, Developer Tools 12\%, Mobile Apps 8\%, Scientific Computing 8\%, Game Development 7\%), and license distribution (MIT 28\%, Apache‑2.0 22\%, GPL variants 18\%, BSD variants 12\%, Proprietary 8\%, Custom/Other 12\%). Projects were further filtered for quality, retaining only those with active development (commits within the last two years), a minimum of 10 stars and 5 contributors, complete dependency information, and valid license metadata (SPDX-compliant or clearly documented). For each selected project, we extracted full dependency trees including direct and transitive dependencies, license information for each dependency, version constraints and compatibility requirements, and package metadata from relevant package managers such as npm, PyPI, and Maven.

Hence, the final dataset provides a multi-dimensional view, comprising 4,000 projects with an average of 127 dependencies each. 
%(spanning from as few as 5 to as many as 2,847). 
It covers more than 750 distinct license types, including widely used open-source licenses, proprietary licenses, and custom variants, and incorporates over 20,000 unique dependencies with complete licensing metadata. In addition, the dataset includes more than 15,000 license compatibility pairs that require analysis and reflects a broad spectrum of project complexity, from single-license applications to complex multi-license enterprise systems.

To establish accurate ground truth for the evaluation of license compatibility, we combined expert validation with authoritative cross-referencing. Three legal professionals specializing in software licensing independently reviewed 177 randomly selected compatibility scenarios, ensuring legal accuracy and consistency. Their evaluations were cross-checked against authoritative sources, including the OSADL (Open Source Automation Development Lab) compatibility matrix, SPDX (Software Package Data Exchange) guidelines, recommendations from the Free Software Foundation, and assessments by the Open Source Initiative. In cases where discrepancies arose between sources, resolution was achieved through legal precedent analysis and expert consensus. This process provided validated ground truth that convered 95\% of license combinations in the dataset, with the remaining cases designated as "ambiguous" for further analysis.


\begin{comment}
\subsection{Baselines}


In our experimental evaluation, we relied on the same prototype baselines previously introduced and detailed in Table~\ref{tab:related_work_comparison}. These baselines were selected to ensure consistency with prior work and to provide a fair comparison framework for assessing the effectiveness of our approach. Each baseline thus serves as a solid benchmark, allowing us to highlight improvements and limitations in a controlled and comparable setting.
\end{comment}

\begin{comment}
    
\begin{table}[ht!]
\centering
\caption{Summary of License Analysis Tools}
\begin{tabular}{|p{3.2cm}|p{11cm}|}
\hline
\multicolumn{2}{|c|}{\textbf{Rule-Based Tools (No Training Required)}} \\
\hline
\textbf{FOSSology v4.2.0} \cite{Gobeille08} & Configured with standard license detection agents (nomos, monk, ninka) and copyright analysis. Used web interface for batch processing of projects with comprehensive reporting. \\
\hline
\textbf{ScanCode Toolkit v32.1.0} \cite{scancode2021} & Deployed with default license detection rules, copyright scanning, and package manifest analysis. Configured for comprehensive license identification using keyword matching and regular expressions. \\
\hline
\textbf{Ninka v1.3.2} \cite{GermanMI10} & Implements sentence-matching method for automatic license identification using regular expressions and pattern matching against known license templates. \\
\hline
\textbf{FLICT} \cite{flict2025} & Implements compatibility checking algorithm using OSADL compatibility matrix with transitive closure computation for multi-license scenarios. \\
\hline
\multicolumn{2}{|c|}{\textbf{Machine Learning-Based Tools}} \\
\hline
\textbf{LiDetector} \cite{DXuGFLLJ23} & Re-implemented using NER+PCFG approach from original paper specifications, trained on 2,400 projects with validation on 800 projects for hyperparameter optimization. \\
\hline
\textbf{OSS-LCAF} \cite{KaholTA25} & Implements comprehensive framework for detecting license conflicts in open source software ecosystems using statistical analysis with ML techniques. \\
\hline
\textbf{ClauseBench} \cite{KeHZW25} & Implements comprehensive benchmark for evaluating ML approaches to software license analysis with standardized datasets and evaluation metrics. \\
\hline
\textbf{SCANOSS} \cite{scanoss_engine2025} & Applies classical ML techniques for software component identification and license analysis using trained statistical models for real-time processing. \\
\hline
\textbf{ContractEval} \cite{liu2025contracteval} & Implements LLM benchmarking framework for contract and license analysis with fine-grained evaluation metrics. \\
\hline
\multicolumn{2}{|c|}{\textbf{Large Language Model-Based Tools}} \\
\hline
\textbf{LiCoEval} \cite{xu2024licoeval} & Implements comprehensive benchmark for evaluating large language models on license compliance in code generation with systematic evaluation metrics. \\
\hline
\textbf{LicenseGPT} \cite{tan2024licensegpt} & Implements fine-tuned foundation model specifically designed for publicly available dataset license compliance analysis with domain-specific fine-tuning. \\
\hline
\textbf{L3icNexus} \cite{CuiW0LYO25} & Implements effective tool for automatically detecting license conflicts using LLMs with AdaFine approach combining DAPT and SFT. \\
\hline
\end{tabular}
\label{tab:license_tools}
\end{table}
\end{comment}




\begin{comment}
\subsection{Evaluation Metrics}
\label{sec:eval_metrics}
To ensure clarity and reproducibility, we define all metrics reported in this paper and describe how they are computed and aggregated.

\paragraph{License Detection Accuracy} The proportion of dependencies for which a tool correctly identifies the governing license. Formally, Accuracy $= (\text{True Positives} + \text{True Negatives}) / \text{All Cases}$. For tools that output a single best license per artifact, we treat the prediction as correct if it matches the ground-truth SPDX identifier or an equivalent license expression.

\paragraph{Compatibility F1} The harmonic mean of precision and recall for binary compatibility conflict detection between license pairs (conflict vs no-conflict). Precision $= \text{TP}/(\text{TP}+\text{FP})$, Recall $= \text{TP}/(\text{TP}+\text{FN})$, and F1 $= 2\cdot \text{Precision}\cdot \text{Recall}/(\text{Precision}+\text{Recall})$. We micro-average across all evaluated license pairs.

\paragraph{False Positive Rate (FPR)} The percentage of non-conflicting pairs incorrectly flagged as conflicts: FPR $= \text{FP}/(\text{FP}+\text{TN})$.

\paragraph{False Negative Rate (FNR)} The percentage of true conflicts missed by the detector: FNR $= \text{FN}/(\text{FN}+\text{TP})$.

\paragraph{Processing Speed} Throughput measured as MB/s or dependencies per second during analysis on the same hardware profile. We report the median over repeated runs on stratified project subsets.

\paragraph{Memory Usage} Peak resident set size (RSS) during end-to-end analysis, measured via system monitors, reported as the median of three runs.

\paragraph{Update Speed} Time needed to incorporate new or custom licenses into the system until they are usable in analysis. For rule-based tools, this includes rule authoring and indexing; for ML-based tools, full or partial retraining; for LARK, graph insertion and RAG indexing.

\paragraph{Coverage} The percentage of license types in our dataset for which the tool can produce a compatibility decision without manual post-processing. For LARK, this reflects both matrix-backed and LLM-parsed licenses integrated in the KG.

\paragraph{Retrieval Precision} For RAG retrieval, the fraction of retrieved citation chunks that experts judged relevant to the specific claim made in the explanation. Computed on sampled queries with dual independent annotations and adjudication.

\paragraph{Response Time} End-to-end latency from user query to final answer. We report the median across 50 queries of varying complexity.

\paragraph{Explainability Score} Expert-rated quality of explanations on a 1--5 Likert scale across four dimensions (citation accuracy, legal reasoning, actionability, usefulness). We report the average composite score. For consistency, experts were blinded to the generating system.

\paragraph{Citation Accuracy} The percentage of citations whose source, section/page, and quoted proposition can be independently verified as supporting the stated claim. A citation is counted correct only if both location and legal proposition match.

\paragraph{Hallucination Rate} The share of responses containing factually incorrect statements that are not supported by either the knowledge graph or retrieved sources. A response is labeled hallucinated if any material claim lacks support or contradicts ground truth.

\paragraph{Aggregation and Uncertainty} Unless stated otherwise, we micro-average metrics over all projects/pairs. We report 95\% confidence intervals using non-parametric bootstrap with 1{,}000 resamples. Statistical comparisons use paired t-tests and Wilcoxon signed-rank tests with Bonferroni correction where applicable.

\end{comment}

\subsection{Comparative Analysis}

\textbf{RQ$_1$: How effective is LARK in comparison with existing tools, and what is the individual contribution of each component?}

\noindent\textbf{Approach.} We comprehensively evaluated LARK against 12 baseline tools (Table~\ref{tab:related_work_comparison}), assessing overall effectiveness and analyzing the contribution of its core components through ablation.
All tools were evaluated on the full dataset of 4,000 projects using identical metrics and statistical analysis protocols, with ML-based tools trained on the subsets specified in their original papers.

\begin{table}[ht!]
\centering
\caption{Performance Comparison with Existing Tools (with Statistical Significance)}
\label{tab:performance-comparison}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Tool} & \textbf{Accuracy} & \textbf{Compatibility F1} & \textbf{Custom Licenses} & \textbf{Explainability}  \\
\midrule
\multicolumn{5}{c}{\textbf{Rule-Based Approaches}} \\
\midrule
FOSSology & 89.3\% ± 2.1 & Limited & Manual & Low  \\
ScanCode & 91.0\% ± 2.3 & Basic & Manual & Medium  \\
Ninka & 88.1\% ± 2.5 & None & No & Low  \\
FLICT & External & 89.3\% ± 1.9 & No & Medium  \\
\midrule
\multicolumn{5}{c}{\textbf{Machine Learning Approaches}} \\
\midrule
LiDetector & 93.2\% ± 1.8 & 88.7\% ± 2.1 & Limited & Low \\
OSS-LCAF & 89.7\% ± 2.0 & 91.2\% ± 1.8 & Limited & Medium  \\
ClauseBench & 87.1\% ± 2.4 & Benchmark & Limited & Medium  \\
SCANOSS & 90.4\% ± 2.1 & Basic & Limited & Low \\
ContractEval & 88.9\% ± 2.3 & 86.7\% ± 2.4\% & Limited & High  \\
\midrule
\multicolumn{5}{c}{\textbf{Large Language Model Approaches}} \\
\midrule
LiCoEval & 87.3\% ± 2.5 & 84.2\% ± 2.6 & Yes & Medium  \\
LicenseGPT & 92.1\% ± 1.9 & 89.5\% ± 2.0 & Yes & High  \\
L3icNexus & 85.58\% ± 2.6 & 85.58\% ± 2.7 & Yes & High  \\
\textbf{LARK} & \textbf{98.1\% ± 0.9} & \textbf{96.2\% ± 1.2} & \textbf{Yes} & \textbf{High}  \\
\bottomrule
\end{tabular}
}
\end{table}


\noindent\textbf{Results.} As shown in Table \ref{tab:performance-comparison}, across 4,000 projects, LARK surpasses all 12 baselines across all reported dimensions, reaching 98.1\% license detection accuracy (best baseline: 93.2\%), 96.2\% compatibility F1 (best baseline: 91.2\%), full support for custom licenses (vs. limited or none), and strong explainability rate\footnote{Experts evaluated the quality of explanations on a 1–5 Likert scale along four dimensions: citation accuracy, legal reasoning, actionability, and usefulness. We report the average composite score, with experts blinded to the system that generated each explanation to ensure consistency.}.

\noindent\textbf{Statistical Significance Analysis:} All performance improvements of LARK over the 12 baselines are statistically significant (p $<$ 0.001) based on paired t-tests with Bonferroni correction for multiple comparisons. Effect sizes (Cohen's d) range from 1.2 to 2.8, indicating large practical significance. 95\% confidence intervals confirm that LARK's performance advantages are robust across different evaluation scenarios.

\begin{table}[ht]
\centering
\caption{Component Ablation Study Results}
\label{tab:ablation-study}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{License Accuracy} & \textbf{Custom License Coverage} & \textbf{Explain Score} \\
\midrule
LARK (Full) & 98.1\% & 94\% & 4.8/5 \\
LARK - KG & 89.4\% & 71\% & 4.2/5 \\
LARK - LLM & 95.8\% & 63\% & 4.1/5 \\
LARK - RAG & 97.2\% & 92\% & 2.1/5 \\
\bottomrule
\end{tabular}
}
\end{table}

\noindent\textbf{Ablation Analysis.} We further assess the contribution of each component as shown in Table \ref{tab:ablation-study}. Without the KG, license detection accuracy drops to 89.4\% and causes failures on 23\% of custom pairs. Omitting LLM parsing decreases custom coverage by 31\%, while removing RAG reduces explainability from 4.8/5 to 2.1/5.

The KG structured relationships across 750+ licenses and 20,000+ dependencies, enabling transitive compatibility analysis across complex dependency chains. LLM integration supports zero-shot processing of novel license texts, extracting obligations and permissions without retraining. RAG enhances performance by retrieving relevant legal text from over 25,000 indexed segments of comprehensive legal literature, achieving 92.8\% precision and producing 3.2× more citations than baseline approaches.

\begin{boxK}
\textit{\textbf{Summary for RQ$_1$.} LARK outperforms all 12 baselines, achieving 98.1\% accuracy, 96.2\% compatibility F1, full support for custom licenses, strong explainability, and a 24-hour update speed. Ablation studies highlight the importance of each component: the Knowledge Graph enables structured reasoning, LLM parsing ensures coverage of novel licenses, and RAG enhances explainability.
Specifically, the Knowledge Graph improves accuracy by 8.7\% through structured reasoning, LLM parsing increases custom license coverage by 31\%, and RAG retrieval boosts explainability from 2.1/5 to 4.8/5.}
\end{boxK}




\begin{comment}
%**************
\noindent\textbf{Approach.} We conducted a comprehensive evaluation comparing LARK against \emph{all} baseline tools reported in Table~\ref{tab:related_work_comparison}, spanning rule-based, ML-based, and LLM-based approaches (12 baselines in total). We measure effectiveness (accuracy, compatibility F1, custom license support, explainability, update speed) and analyze component contributions via ablation (KG, LLM parsing, RAG).

\noindent\textbf{Baseline Implementation Details.} To ensure comprehensive comparison, we evaluated LARK against the full set of 12 baseline approaches across multiple categories:

\textbf{Rule-Based Tools:} FOSSology v4.2.0, ScanCode Toolkit v32.1.0, Ninka v1.3.2, FLICT compatibility checker
\textbf{Machine Learning Tools:} LiDetector (NER+PCFG), OSS-LCAF (conflict analysis framework), ClauseBench (SVM/RF), SCANOSS (statistical models), ContractEval (LLM benchmarking)
\textbf{Large Language Model Tools:} LiCoEval (GPT-4 evaluation), LicenseGPT (fine-tuned model), L3icNexus (License-Llama3-8B)

All tools were evaluated on the complete 4,000 OSS project dataset with identical evaluation metrics and statistical analysis protocols. ML-based tools were trained on appropriate subsets as specified in their original papers.

\noindent\textbf{Results.} On 4,000 OSS projects, LARK outperforms \emph{all} 12 baselines across the reported dimensions. LARK achieves 98.1\% license detection accuracy (best baseline: 93.2\%), 96.2\% compatibility F1 (best baseline: 91.2\%), full support for custom licenses (vs limited/no support), high explainability, and 24-hour update speed (vs retraining/days-weeks for baselines). 

\textcolor{black}{\textbf{RQ$_1$} demonstrates that the integrated KG+LLM+RAG approach provides superior performance across all evaluation dimensions. The knowledge graph enables comprehensive relationship modeling between licenses, packages, and obligations. LLM integration allows processing of custom and novel license texts without retraining. RAG enhancement provides detailed, citation-backed explanations that are essential for regulatory compliance and legal decision-making.}


\textbf{Statistical Significance Analysis:} All performance improvements of LARK over the 12 baselines are statistically significant (p < 0.001) based on paired t-tests with Bonferroni correction for multiple comparisons. Effect sizes (Cohen's d) range from 1.2 to 2.8, indicating large practical significance. 95\% confidence intervals confirm that LARK's performance advantages are robust across different evaluation scenarios.

\begin{boxK}
\textit{\textbf{Summary for RQ$_1$.} LARK achieves superior performance vs all 12 baselines (98.1\% accuracy, 96.2\% compatibility F1, full custom license support, high explainability, 24-hour update speed). Ablations show each component is essential: KG (structured reasoning), LLM parsing (coverage of novel licenses), RAG (explainability).}
\end{boxK}


\noindent\textbf{Ablation Analysis.} We further evaluate the contribution of each component. Removing the Knowledge Graph reduces license accuracy to 89.4\% and causes failures on 23\% of custom pairs; removing LLM parsing reduces custom coverage by 31\%; removing RAG reduces explainability from 4.8/5 to 2.1/5.

The Knowledge Graph provides structured relationship modeling across 750+ licenses and 20,000+ dependencies, enabling transitive compatibility analysis across complex dependency chains. LLM integration allows zero-shot processing of novel license texts, extracting obligations and permissions without requiring retraining. RAG enhancement retrieves relevant legal text chunks from 25,000+ indexed segments across comprehensive legal literature with 92.8\% retrieval precision, providing 3.2x more citations compared to baseline approaches.


  
\vspace{-.1cm}
\begin{boxK}
\textit{\textbf{Summary for RQ$_2$.} Each system component provides essential capabilities: Knowledge Graph enables structured reasoning (8.7\% accuracy improvement), LLM parsing provides custom license support (31\% coverage improvement), and RAG retrieval delivers critical explainability (4.8/5 vs 2.1/5 without RAG).}
\end{boxK}



\end{comment}


\textbf{RQ$_2$: How does the framework handle custom and proprietary licenses?}
\noindent\textbf{Approach.} We assessed LARK's capability to handle custom and proprietary licenses using a curated dataset of 100 enterprise licenses, including modified standard licenses, entirely custom licenses, and proprietary texts. Performance was compared against rule-based and ML-based approaches, which typically require manual configuration or retraining.


\noindent\textbf{Results.} LARK demonstrates strong capability in handling custom and proprietary licenses through its LLM-powered parsing pipeline. Evaluation shows 94\% successful parsing accuracy for custom licenses, compared to 23\% for rule-based tools (requiring extensive manual configuration) and weeks of manual effort for ML-based approaches. The system effectively extracts obligations, prohibitions, and permissions from enterprise licenses, including modified BSD variants, custom attribution requirements, and proprietary distribution restrictions.

Using few-shot learning with 2–3 examples, the LLM parsing component achieves 91\% accuracy in obligation extraction and 88\% in prohibition identification. Parsed license concepts are automatically integrated into the knowledge graph via predefined relationship templates, ensuring consistent representation across license types. Average processing time for custom licenses is 2.3 seconds, dramatically faster than the manual configuration required by traditional tools.

\begin{boxK}
\textit{\textbf{Summary for RQ$_2$.} LARK achieves 94\% accuracy in parsing custom licenses with 2.3-second processing time, compared to 23\% capability for rule-based tools (requiring extensive manual configuration) and weeks of manual effort for ML-based approaches. The LLM parsing enables automatic handling of enterprise and proprietary licenses.}
\end{boxK}


\begin{comment}
    

%***********
\vspace{-.3cm}
\subsection{RQ$_2$: How does the framework handle custom and proprietary licenses?}
\noindent\textbf{Approach.} We evaluated LARK's ability to process custom and proprietary licenses by testing on a curated dataset of 100 enterprise licenses, including modified versions of standard licenses, completely custom licenses, and proprietary license texts. We compared performance against rule-based and ML-based approaches that typically require manual configuration or retraining.

\noindent\textbf{Results.} LARK demonstrates exceptional capability in handling custom and proprietary licenses through its LLM-powered parsing pipeline. Our evaluation shows 94\% successful parsing accuracy for custom licenses, compared to 23\% for rule-based tools (requiring extensive manual configuration) and requiring weeks of manual effort for ML-based approaches. The framework successfully extracted obligations, prohibitions, and permissions from enterprise licenses including modified BSD variants, custom attribution requirements, and proprietary distribution restrictions.

The LLM parsing component employs few-shot learning with 2-3 examples to adapt to custom license formats, achieving 91\% accuracy in obligation extraction and 88\% in prohibition identification. The system automatically integrates parsed license concepts into the knowledge graph using predefined relationship templates, ensuring consistent representation across different license types. Processing time for custom licenses averages 2.3 seconds compared to weeks of manual configuration required by traditional tools.

\textcolor{black}{\textbf{RQ$_2$} demonstrates that LARK's LLM-powered approach provides unprecedented capability for handling custom and proprietary licenses. While traditional tools require manual rule creation or model retraining, our framework processes novel licenses through zero-shot and few-shot learning, achieving 94\% parsing accuracy with automatic knowledge graph integration. This capability is essential for enterprise environments where custom licenses are common but traditional tools provide no support.}

\begin{boxK}
\textit{\textbf{Summary for RQ$_2$.} LARK achieves 94\% accuracy in parsing custom licenses with 2.3-second processing time, compared to 23\% capability for rule-based tools (requiring extensive manual configuration) and weeks of manual effort for ML-based approaches. The LLM parsing enables automatic handling of enterprise and proprietary licenses.}
\end{boxK}
%\input{Charts/LicenseOperations}

\begin{figure}
\centering 
\includegraphics[width=0.6\columnwidth]{Images/wordcloud8.PNG}
\caption{Popular license compatibility textual patterns in issues.}
\label{fig:Top Keywords}
\end{figure}

\end{comment}
% [RQ on operational efficiency removed per consolidation request]





\textbf{RQ$_3$: How well does RAG perform in terms of explanation quality compared to all baselines?}
\noindent\textbf{Approach.}
We implemented a comprehensive, multi-dimensional explainability evaluation framework that combines LLM-based automated assessment with expert validation. Our methodology employs a fine-tuned GPT-4 evaluator trained on 1,000 expert-annotated legal explanations to assess explanation quality across four dimensions: citation accuracy, legal reasoning quality, actionable recommendations, and overall usefulness. Each explanation is evaluated using structured prompts targeting these specific criteria, and expert validation on 200 randomly selected explanations confirmed 95.2\% agreement with the LLM evaluator. The framework captures fine-grained aspects of explanation quality. Citation accuracy considers the presence of verifiable legal sources, correct page numbers, proper SPDX identifiers, and accurate legal precedents. Legal reasoning quality evaluates sound logic, correct license interpretation, proper application of legal principles, and coherent argumentation. Actionable recommendations measure the specificity of remediation steps, alternative licensing strategies, compliance guidance, and practical solutions. Overall usefulness assesses comprehensiveness, clarity, relevance to user needs, and suitability for legal decision-making. Training data consisted of a curated dataset of license compatibility explanations with expert ratings on a 1–5 scale, enabling the LLM evaluator to provide precise and reliable assessments of explanation quality across these dimensions.
We assessed explanations generated by LARK and all 12 baselines across 177 randomly selected compatibility scenarios, with each explanation evaluated by an LLM and validated by legal experts.

\begin{table}[ht]
\centering
\caption{LLM-Based Explainability Evaluation Results}
\label{tab:explanation-quality}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Tool} & \textbf{Explainability} & \textbf{Citation Count} & \textbf{Citation Relevance} & \textbf{Legal Reasoning} \\
\midrule
FOSSology & Low & 0.2 ± 0.1 & 35\% ± 10\% & 1.5/5 ± 0.3 \\
ScanCode & Medium & 0.5 ± 0.2 & 45\% ± 12\% & 2.1/5 ± 0.4 \\
Ninka & Low & 0.1 ± 0.1 & 25\% ± 8\% & 1.2/5 ± 0.3 \\
FLICT & Medium & 1.8 ± 0.3 & 78\% ± 6\% & 3.5/5 ± 0.3 \\
LiDetector & Low & 1.8 ± 0.4 & 67\% ± 8\% & 2.9/5 ± 0.4 \\
OSS-LCAF & Medium & 2.1 ± 0.3 & 72\% ± 7\% & 3.2/5 ± 0.3 \\
ClauseBench & Medium & 1.9 ± 0.3 & 69\% ± 8\% & 3.1/5 ± 0.3 \\
SCANOSS & Low & 0.8 ± 0.2 & 52\% ± 10\% & 2.2/5 ± 0.4 \\
ContractEval & High & 3.2 ± 0.5 & 85\% ± 5\% & 3.8/5 ± 0.2 \\
LiCoEval & Medium & 2.8 ± 0.4 & 78\% ± 6\% & 3.1/5 ± 0.4 \\
LicenseGPT & High & 4.1 ± 0.6 & 89\% ± 4\% & 3.9/5 ± 0.2 \\
L3icNexus & High & 3.8 ± 0.5 & 87\% ± 5\% & 3.7/5 ± 0.3 \\
\textbf{LARK} & \textbf{High} & \textbf{5.7 ± 0.8} & \textbf{96\% ± 3\%} & \textbf{4.9/5 ± 0.1} \\
\bottomrule
\end{tabular}
}
\end{table}


\noindent\textbf{Results.} LARK's RAG-enhanced explanations significantly outperform all 12 baselines across every evaluation dimension. On 177 randomly selected compatibility scenarios, the LLM evaluator\footnote{Expert validation on 200 randomly selected explanations confirms 95.2\% agreement with LLM evaluator ratings across all dimensions} rated LARK explanations 4.8/5 overall, compared to 3.2/5 for LiDetector and 2.1/5 for rule-based tools, with expert validation confirming 95.2\% agreement with LLM assessments. For citation accuracy, LARK achieved 4.9/5 with 96\% verifiable citations, while LiDetector scored 2.8/5 (67\% verifiable) and rule-based tools 1.9/5 (45\% verifiable); the evaluator highlighted LARK's precise page numbers, section references, and SPDX identifiers. In legal reasoning quality, LARK scored 4.9/5 for sound legal logic and license interpretation versus 2.9/5 for LiDetector and 1.8/5 for rule-based tools, demonstrating clear explanations of complex compatibility scenarios with proper legal and regulatory context. For actionable recommendations, LARK achieved 4.7/5, surpassing LiDetector (3.1/5) and rule-based tools (2.0/5), providing specific remediation steps and alternative licensing strategies. Overall usefulness was rated 4.8/5 for LARK, compared to 3.2/5 for LiDetector and 2.1/5 for rule-based tools, with explanations detailed enough for legal decision-making and compliance documentation. The fine-tuned LLM evaluator itself showed high reliability, agreeing 95.2\% with expert ratings across all dimensions. Automated metrics confirmed these findings: LARK provides an average of 5.7 citations per explanation (vs. 1.8 for LiDetector), 96\% citation precision (vs. 67\%), comprehensive explanations averaging 387 words (vs. 156), and 98.1\% technical accuracy (vs. 93.2\%).

\begin{boxK}
\textit{\textbf{Summary for RQ$_3$.} Combining RAG with KG produces high-quality, citation-backed explanations (5.7 citations per explanation, 96\% relevance, 4.9/5 legal reasoning).}
\end{boxK}


\begin{comment}
    
%**************
\subsection{RQ$_3$: How well does RAG perform in terms of explanation quality and hallucination reduction compared to all baselines?}
\noindent\textbf{Approach.} We implemented a comprehensive multi-dimensional explainability evaluation framework combining LLM-based automated assessment with expert validation. Our evaluation methodology employs a fine-tuned LLM evaluator trained on legal reasoning benchmarks to assess explanation quality across four key dimensions: citation accuracy, legal reasoning quality, actionable recommendations, and overall usefulness.

\textbf{LLM-Based Evaluation Framework:}
\begin{itemize}
    \item \textbf{Evaluator Model:} Fine-tuned GPT-4 model trained on 1,000 expert-annotated legal explanations with ground truth ratings across all evaluation dimensions
    \item \textbf{Training Data:} Curated dataset of license compatibility explanations with expert ratings (1-5 scale) for citation accuracy, legal reasoning, actionability, and usefulness
    \item \textbf{Evaluation Protocol:} Each explanation evaluated by the LLM evaluator using structured prompts that assess specific criteria for each dimension
    \item \textbf{Validation:} Expert validation on 200 randomly selected explanations to ensure LLM evaluator accuracy (95.2\% agreement with expert ratings)
\end{itemize}

\textbf{Evaluation Dimensions and Criteria:}
\begin{itemize}
    \item \textbf{Citation Accuracy:} Presence of verifiable legal sources, correct page numbers, proper SPDX identifiers, and accurate legal precedents
    \item \textbf{Legal Reasoning Quality:} Sound legal logic, proper license interpretation, correct application of legal principles, and coherent argumentation
    \item \textbf{Actionable Recommendations:} Specific remediation steps, alternative licensing strategies, compliance guidance, and practical solutions
    \item \textbf{Overall Usefulness:} Comprehensiveness, clarity, relevance to user needs, and suitability for legal decision-making
\end{itemize}

\textbf{Comparative Analysis:} We evaluated explanations from LARK and \emph{all} 12 baselines on 177 randomly selected compatibility scenarios, with each explanation rated by the LLM evaluator and validated by legal experts.

\noindent\textbf{Results.} LARK's RAG-enhanced explanations significantly outperform existing approaches across all evaluation dimensions. The LLM evaluator rates our explanations at 4.8/5 overall compared to 3.2/5 for LiDetector and 2.1/5 for rule-based tools, with expert validation confirming 95.2\% agreement with LLM assessments.

\textbf{Detailed Evaluation Results:}
\begin{itemize}
    \item \textbf{Citation Accuracy:} LARK achieves 4.9/5 rating with 96\% verifiable citations, compared to 2.8/5 for LiDetector (67\% verifiable) and 1.9/5 for rule-based tools (45\% verifiable). The LLM evaluator specifically noted LARK's precise page numbers, section references, and SPDX identifiers for all legal sources.
    \item \textbf{Legal Reasoning Quality:} LARK scores 4.9/5 for sound legal logic and proper license interpretation, compared to 2.9/5 for LiDetector and 1.8/5 for rule-based tools. The evaluator highlighted LARK's ability to explain complex compatibility scenarios with clear legal precedents and regulatory context.
    \item \textbf{Actionable Recommendations:} LARK achieves 4.7/5 for practical solutions and compliance guidance, compared to 3.1/5 for LiDetector and 2.0/5 for rule-based tools. The evaluator appreciated specific remediation steps and alternative licensing strategies.
    \item \textbf{Overall Usefulness:} LARK receives 4.8/5 for general utility and comprehensiveness, compared to 3.2/5 for LiDetector and 2.1/5 for rule-based tools. The evaluator noted that LARK explanations provide sufficient detail for legal decision-making and regulatory compliance documentation.
\end{itemize}

\textbf{LLM Evaluator Performance:} The fine-tuned evaluator demonstrates high accuracy with 95.2\% agreement with expert ratings across all dimensions. Automated metrics confirm expert assessments: LARK provides 5.7 citations per explanation vs 1.8 for LiDetector, achieves 96\% citation precision vs 67\% for LiDetector, generates comprehensive explanations (average 387 words) vs 156 words for LiDetector, and maintains 98.1\% technical accuracy vs 93.2\% for LiDetector.

\textcolor{black}{\textbf{RQ$_3$} demonstrates that RAG-enhanced explanations provide superior quality across all evaluation dimensions while simultaneously mitigating hallucinations. The system generates explanations that include precise citations (5.7 per explanation vs 1.8 for baselines), comprehensive legal reasoning (4.8/5 expert rating vs 3.2/5), and actionable compliance recommendations, with significantly reduced hallucination rate due to KG constraints and citation grounding.}

\begin{table}[ht]
\centering
\caption{LLM-Based Explainability Evaluation Results}
\label{tab:explanation-quality}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Tool} & \textbf{Explainability} & \textbf{Citation Count} & \textbf{Citation Relevance} & \textbf{Legal Reasoning} \\
\midrule
FOSSology & Low & 0.2 ± 0.1 & 35\% ± 10\% & 1.5/5 ± 0.3 \\
ScanCode & Medium & 0.5 ± 0.2 & 45\% ± 12\% & 2.1/5 ± 0.4 \\
Ninka & Low & 0.1 ± 0.1 & 25\% ± 8\% & 1.2/5 ± 0.3 \\
FLICT & Medium & 1.8 ± 0.3 & 78\% ± 6\% & 3.5/5 ± 0.3 \\
LiDetector & Low & 1.8 ± 0.4 & 67\% ± 8\% & 2.9/5 ± 0.4 \\
OSS-LCAF & Medium & 2.1 ± 0.3 & 72\% ± 7\% & 3.2/5 ± 0.3 \\
ClauseBench & Medium & 1.9 ± 0.3 & 69\% ± 8\% & 3.1/5 ± 0.3 \\
SCANOSS & Low & 0.8 ± 0.2 & 52\% ± 10\% & 2.2/5 ± 0.4 \\
ContractEval & High & 3.2 ± 0.5 & 85\% ± 5\% & 3.8/5 ± 0.2 \\
LiCoEval & Medium & 2.8 ± 0.4 & 78\% ± 6\% & 3.1/5 ± 0.4 \\
LicenseGPT & High & 4.1 ± 0.6 & 89\% ± 4\% & 3.9/5 ± 0.2 \\
L3icNexus & High & 3.8 ± 0.5 & 87\% ± 5\% & 3.7/5 ± 0.3 \\
\textbf{LARK} & \textbf{High} & \textbf{5.7 ± 0.8} & \textbf{96\% ± 3\%} & \textbf{4.9/5 ± 0.1} \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{LLM Evaluator Validation:} Expert validation on 200 randomly selected explanations confirms 95.2\% agreement with LLM evaluator ratings across all dimensions (Cronbach's alpha > 0.85), demonstrating the reliability of our automated evaluation approach.

% \end{quote}





\begin{boxK}
\textit{\textbf{Summary for RQ$_3$.} RAG + KG yields high-quality, citation-backed explanations (5.7 citations/explanation; 96\% relevance; 4.9/5 legal reasoning) and markedly lowers hallucination compared to all 12 baselines.}
\end{boxK}


\end{comment}


\textbf{RQ$_4$: How effective is RAG in  mitigating hallucinations compared to all baselines?}


% RQ on hallucination is merged into RQ3 above
\noindent\textbf{Approach.} We evaluated LARK's hallucination mitigation capabilities by analyzing how the framework constrains LLM decision-making through structured knowledge graph outputs and RAG-grounded explanations. We compared LARK's constrained approach against unconstrained LLM responses and measured hallucination rates across different scenarios including novel license combinations, ambiguous compatibility cases, and edge cases not covered in training data.

\noindent\textbf{Results.} LARK demonstrates exceptional hallucination mitigation through its dual-constraint architecture. According to Table \ref{tab:hallucination-mitigation}, the framework achieves a 2.1\% hallucination rate compared to 18.7\% for unconstrained LLM responses and 8.3\% for traditional ML approaches. The knowledge graph component provides structured compatibility decisions that the LLM cannot override, ensuring factual accuracy in compatibility determinations. The RAG system grounds all explanations in authoritative legal sources, reducing hallucination in explanatory text by 89\% compared to LLM-only approaches.

The constrained architecture operates through two key mechanisms: (1) \textit{Knowledge Graph Constraint}: The LLM receives pre-computed compatibility results from the knowledge graph and is instructed to explain these results rather than generate new compatibility decisions, eliminating the possibility of hallucinated compatibility judgments; (2) \textit{RAG Grounding}: All explanations are generated using retrieved legal text chunks as context, ensuring that explanations reference actual legal sources rather than generating fictional legal reasoning. Expert evaluation confirms that 97.8\% of LARK's responses contain verifiable citations, compared to 23.4\% for unconstrained LLM responses.

%\textcolor{black}{\textbf{RQ$_6$} demonstrates that LARK's architecture effectively mitigates LLM hallucination through structured constraints and authoritative grounding. The knowledge graph provides factual compatibility decisions that cannot be hallucinated, while the RAG system ensures all explanations are grounded in real legal sources. This dual-constraint approach is essential for regulatory compliance where accuracy and verifiability are paramount, addressing a critical limitation of current LLM-based legal analysis tools.}

\begin{table}[ht]
\centering
\caption{Hallucination Mitigation Analysis}
\label{tab:hallucination-mitigation}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Hallucination Rate} & \textbf{Verifiable Citations} & \textbf{Factual Accuracy} & \textbf{Expert Confidence} \\
\midrule
Unconstrained LLM & 18.7\% & 23.4\% & 67.2\% & 2.1/5 \\
Traditional ML & 8.3\% & 45.6\% & 78.9\% & 3.2/5 \\
LARK (Full) & \textbf{2.1\%} & \textbf{97.8\%} & \textbf{96.2\%} & \textbf{4.8/5} \\
LARK - KG Constraint & 12.4\% & 89.3\% & 87.1\% & 3.9/5 \\
LARK - RAG Grounding & 6.7\% & 94.2\% & 91.8\% & 4.2/5 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{boxK}
\textit{\textbf{Summary for RQ$_4$.} LARK achieves exceptional hallucination mitigation with 2.1\% hallucination rate (vs 18.7\% for unconstrained LLM) through dual-constraint architecture: knowledge graph provides factual compatibility decisions that LLM cannot override, while RAG system grounds all explanations in authoritative legal sources with 97.8\% verifiable citations.}
\end{boxK}

