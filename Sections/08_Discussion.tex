\section{Discussion}
\label{Section:Discussion}
The LARK framework represents a significant advancement in automated license compatibility detection by integrating Knowledge Graphs, Large Language Models, and Retrieval-Augmented Generation. Our comprehensive evaluation demonstrates that this integrated approach addresses critical limitations in existing tools while providing superior performance across accuracy, explainability, and operational efficiency dimensions. This section discusses key insights from our research and implications for future license compliance systems.



\noindent\textbf{ Takeaway \#1: \textit{Knowledge Graph integration is essential for comprehensive license compatibility analysis.}} Our evaluation demonstrates that the Knowledge Graph component contributes critically to overall system effectiveness, providing 8.7\% accuracy improvement over LLM-only approaches. The structured representation across 750+ licenses and 20,000+ dependencies enables transitive compatibility analysis across complex dependency chains and supports rapid updates when new licenses emerge. The graph-based approach allows for systematic reasoning about license relationships that would be difficult to achieve through pattern matching or ML classification alone. Results from \textbf{RQ$_1$} and \textbf{RQ$_2$} show that removing the Knowledge Graph component reduces license detection accuracy to 89.4\% and fails compatibility analysis for 23\% of custom license combinations.

\noindent\textbf{ Takeaway \#2: \textit{LLM-powered custom license parsing enables unprecedented extensibility.}} Traditional license analysis tools struggle with custom and proprietary licenses due to their reliance on predefined patterns or trained models that require extensive retraining. Our LLM-based parsing approach achieves 94\% accuracy in processing custom licenses with 2.3-second processing time, compared to 0\% capability for rule-based tools. As demonstrated in \textbf{RQ$_2$}, the few-shot learning approach with 2-3 examples enables automatic adaptation to novel license formats, extracting obligations, prohibitions, and permissions without requiring model retraining. This capability is essential for enterprise environments where custom licenses are common but existing tools provide no support.



\noindent\textbf{ Takeaway \#3: \textit{RAG-enhanced explanations are crucial for regulatory compliance and legal decision-making.}} Expert evaluation reveals that LARK's RAG-enhanced explanations significantly outperform existing approaches, achieving 4.8/5 rating compared to 3.2/5 for LiDetector and 2.1/5 for rule-based tools. The system provides 3.2x more relevant citations per explanation (5.7 vs 1.8 for baselines) with 96\% citation relevance. As demonstrated in \textbf{RQ$_3$}, the RAG system retrieves relevant legal text chunks from 25,000+ indexed segments across comprehensive legal literature including authoritative books, IEEE/ACM articles, and regulatory guidelines with 92.8\% precision, enabling explanations that reference specific license clauses, legal precedents, regulatory requirements, and authoritative legal sources with exact page numbers and section references. This explainability capability addresses a critical gap in existing tools where binary compatibility decisions lack detailed justification, creating significant challenges for regulatory compliance where audit trails and justification documentation are essential.

\noindent\textbf{ Takeaway \#4: \textit{Dual-constraint architecture effectively mitigates LLM hallucination in legal analysis.}} As demonstrated in \textbf{RQ$_4$}, LARK's architecture addresses a critical limitation of LLM-based legal tools through structured constraints that prevent hallucination. The knowledge graph provides factual compatibility decisions that the LLM cannot override, ensuring that compatibility judgments are based on structured relationships rather than generated content. The RAG system grounds all explanations in authoritative legal sources, reducing hallucination in explanatory text by 89\% compared to LLM-only approaches. This dual-constraint approach achieves a 2.1\% hallucination rate compared to 18.7\% for unconstrained LLM responses, with 97.8\% of responses containing verifiable citations. This architecture is essential for regulatory compliance where accuracy and verifiability are paramount, addressing concerns about LLM reliability in legal decision-making contexts.

   % \noindent\textbf{ Takeaway \#7: \textit{There is potential data leakages between data used to train LLMs and data used to evaluate research. This must be controlled for/mitigated in research studies.}} %\eman{This is an implication (not takeaway), and it might sounds generic, would you recommend keeping it or removing it?}.}} 
   % \textcolor{red}{%The manner of LLM expressions of confidence or apologies reported in \textbf{RQ$_5$} may impact the emergence of certain types of issues, like data leakage, in software development projects. 
   %  A recent study has shed light on the potential risks associated with explicit or implicit data leakage between LLMs training data and research evaluation \cite{sallou2023breaking}. One identified threat is the possibility of data leakage, which arises from the blurred separation of training, validation, and test sets \cite{yang2022data}. While this concern is recognized in LLM-related research, it is particularly relevant in the context of utilizing LLMs for software engineering tasks, such as license analysis. To address this challenge, we propose a recommendation for future studies using LLMs for license compatibility tasks. Specifically, researchers should consider using recent GitHub metadata, including license-related commits, issues, pull requests, and other relevant data, as a test dataset. This recommendation is based on recent findings that explored the types of tasks generated by LLMs \cite{tufano2024unveiling}. By doing so, researchers can mitigate the risk of data leakage, as the model behind LLMs is unlikely to have encountered these data during its training phase, ensuring data integrity and reliability.}

  