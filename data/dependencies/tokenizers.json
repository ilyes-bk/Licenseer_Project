{
    "code_of_conduct_url": null,
    "contribution_guidelines_url": null,
    "dependent_repos_count": 424,
    "dependents_count": 870,
    "deprecation_reason": null,
    "description": "ðŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production",
    "homepage": "https://github.com/huggingface/tokenizers",
    "keywords": [
        "NLP",
        "tokenizer",
        "BPE",
        "transformer",
        "deep",
        "learning",
        "bert",
        "gpt",
        "language-model",
        "natural-language-processing",
        "natural-language-understanding",
        "transformers"
    ],
    "language": "Rust",
    "latest_release_number": "0.21.1",
    "latest_release_published_at": "2025-03-13T10:50:56.000Z",
    "latest_stable_release_number": "0.21.1",
    "latest_stable_release_published_at": "2025-03-13T10:50:56.000Z",
    "license_normalized": true,
    "licenses": "Apache Software License",
    "name": "tokenizers",
    "normalized_licenses": [
        "Apache-2.0"
    ],
    "package_manager_url": "https://pypi.org/project/tokenizers/",
    "repository_license": "Apache-2.0",
    "platform": "Pypi",
    "repository_status": null,
    "repository_url": "https://github.com/huggingface/tokenizers",
    "security_policy_url": null
}